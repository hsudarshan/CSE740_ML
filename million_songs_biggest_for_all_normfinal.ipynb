{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PLOT test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.plot([300.0,301.0,302.0,303.0], [1,7,12,16], 'ro')\n",
    "plt.axis([290, 310, 0, 20])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%fs ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "\n",
    "#sparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/vd5hrrua1480026865018/\",header='true')\n",
    "\n",
    "sparkDF = sqlContext.read.format(\"com.databricks.spark.csv\").load(\"gs://millionsongsoutput/dataABC/*.csv\",header='true')\n",
    "\"\"\"\n",
    "sparkDF1 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/unfd0khf1480700528583/\",header='true')#A\n",
    "sparkDF2 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/i9k46ctm1480700408609/\",header='true')#B\n",
    "sparkDF3 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/uzzxeqtc1480659570445/\",header='true')#C\n",
    "sparkDF4 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/zh8w7cvo1481174995446/\",header='true')#D\n",
    "sparkDF5 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/cmcoiahn1481175345159/\",header='true')#E\n",
    "sparkDF6 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/uu0j5t8x1481175546319/\",header='true')#F\n",
    "\n",
    "\n",
    "\n",
    "appended = sparkDF1.unionAll(sparkDF2)\n",
    "\n",
    "appended = appended.unionAll(sparkDF3)\n",
    "appended = appended.unionAll(sparkDF4)\n",
    "appended = appended.unionAll(sparkDF5)\n",
    "sparkDF=appended.unionAll(sparkDF3)\n",
    "\"\"\"\n",
    "\n",
    "print(sparkDF.count())\n",
    "print(sparkDF.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "sparkDF.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "#x[\"Danceability\"],x[\"ArtistLatitude\"],x[\"ArtistLongitude\"],x[\"SongNumber\"]\n",
    "sparkDF_new=sparkDF.map(lambda x:[x[\"song_hotttnesss\"],x[\"artist_hotttnesss\"],x[\"artist_familiarity\"],x[\"Tempo\"],x[\"KeySignature\"],x[\"KeySignatureConfidence\"],x[\"TimeSignature\"],x[\"TimeSignatureConfidence\"],x[\"Year\"],x[\"timbre0\"],x[\"timbre1\"],x[\"timbre2\"],x[\"timbre3\"],x[\"timbre4\"],x[\"timbre5\"],x[\"timbre6\"],x[\"timbre7\"],x[\"timbre8\"],x[\"timbre9\"],x[\"timbre10\"],x[\"timbre11\"],x[\"Duration\"]])\n",
    "\n",
    "\n",
    "#,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(sparkDF_new))\n",
    "sparkDF_new.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_filter_bf=sparkDF_new.filter(lambda x:x[0]!='nan' and x[1]!='null' and x[2]!='null' and x[3]!='null' and x[4]!='null' and x[5]!='null' and x[6]!='null' and x[7]!='null' and x[8]!='null' and x[9]!='null' and x[10]!='null' and x[11]!='null' and x[12]!='null' and x[13]!='null' and x[14]!='null' and x[15]!='null' and x[16]!='null' and x[17]!='null' and x[18]!='null' and x[19]!='null' and x[20]!='null' and x[21]!='null')\n",
    "\n",
    "# \n",
    "\n",
    "sparkDF_new_filter=sparkDF_new_filter_bf.filter(lambda x:x[0]!='nan' and x[1]!='nan' and x[2]!='nan' and x[3]!='nan' and x[4]!='nan' and x[5]!='nan' and x[6]!='nan' and x[7]!='nan' and x[8]!='nan' and x[9]!='nan' and x[10]!='nan' and x[11]!='nan' and x[12]!='nan' and x[13]!='nan' and x[14]!='nan' and x[15]!='nan' and x[16]!='nan' and x[17]!='nan' and x[18]!='nan' and x[19]!='nan' and x[20]!='nan' and x[21]!='nan')\n",
    "\n",
    "\n",
    "\n",
    "sparkDF_new_filter.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHECK FOR EMPTY COLUMNS\n",
    "\n",
    "#Artisthotness GOOD\n",
    "#artistfamilarity GOOD\n",
    "#ArtistLatitude 2196 remains\n",
    "#ArtistLongitude 2196 remains\n",
    "#tempo GOOD\n",
    "#KeySignature GOOD\n",
    "#KeySignatureConfidence GOOD\n",
    "#TimeSignature GOOD\n",
    "#TimeSignatureConfidence GOOD\n",
    "#Year GOOD\n",
    "#Timbre0-11 GOOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_filter_test=sparkDF_new_filter.filter(lambda x:x[0]!=\"\" and x[1]!=\"\" and x[2]!=\"\" and x[3]!=\"\" and x[4]!=\"\" and x[5]!=\"\" and x[6]!=\"\" and x[7]!=\"\" and x[8]!=\"\" and x[9]!=\"\" and x[10]!=\"\" and x[11]!=\"\" and x[12]!=\"\" and x[13]!=\"\" and x[14]!=\"\" and x[15]!=\"\" and x[16]!=\"\" and x[17]!=\"\" and x[18]!=\"\" and x[19]!=\"\" and x[20]!=\"\" and x[20]!=\"\" and x[21]!=\"\")\n",
    "empty_filter_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_filter_test.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f=empty_filter_test.map(lambda x:[float(x[0]),float(x[1]),float(x[2]),float(x[3]),float(x[4]),float(x[5]),float(x[6]),float(x[7]),float(x[8]),float(x[9]),float(x[10]),float(x[11]),float(x[12]),float(x[13]),float(x[14]),float(x[15]),float(x[16]),float(x[17]),float(x[18]),float(x[19]),float(x[20]),float(x[21])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHECK FOR 0 COLUMNS\n",
    "\n",
    "#Artisthotness GOOD 4200\n",
    "#artistfamilarity GOOD\n",
    "#ArtistLatitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#ArtistLongitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#tempo GOOD\n",
    "#KeySignature STILL GOOD\n",
    "#KeySignatureConfidence STILL GOOD\n",
    "#TimeSignature STILL GOOD\n",
    "#TimeSignatureConfidence GOOD 3300\n",
    "#Year NOT GOOD 2700 we need to ignore 0 if we need to proceed\n",
    "###########Timbre0-11 CAN HAVE NEGATIVE VALUES\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# [x[\"song_hotttnesss\"],x[\"artist_hotttnesss\"],x[\"artist_familiarity\"],x[\"Tempo\"],x[\"KeySignature\"],x[\"KeySignatureConfidence\"],x[\"TimeSignature\"],x[\"TimeSignatureConfidence\"],x[\"Year\"],x[\"timbre0\"],x[\"timbre1\"],x[\"timbre2\"],x[\"timbre3\"],x[\"timbre4\"],x[\"timbre5\"],x[\"timbre6\"],x[\"timbre7\"],x[\"timbre8\"],x[\"timbre9\"],x[\"timbre10\"],x[\"timbre11\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f_above_0_hot=sparkDF_new_f.filter(lambda x:x[0]>0.0 and x[8]>0.0)\n",
    "sparkDF_new_f_above_0_hot.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparkDF_new_f=sparkDF_new_filter.map(lambda x:[float(x[0]),float(x[1]),float(x[2]),float(x[3]),float(x[4]),float(x[5]),float(x[6]),float(x[7]),float(x[8]),float(x[9]),float(x[10]),float(x[11]),float(x[12]),float(x[13]),float(x[14])])\n",
    "\n",
    "\n",
    "#,float(x[15]),float(x[16]),float(x[17]),float(x[18]),float(x[19]),float(x[20]),float(x[21]),float(x[22])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(sparkDF_new_f.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "def tolabelpoint(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "  return LabeledPoint(line[0],line[1:])\n",
    "\n",
    "sparkDF_new_f_label=sparkDF_new_f_above_0_hot.map(lambda x:tolabelpoint(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f_label.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data=sparkDF_new_f_label.filter(lambda x:x.label>0.0 and x.features>0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=sqlContext.createDataFrame(test_filtered_data.collect())\n",
    "\n",
    "test_filtered_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_labels=test_filtered_data.map(lambda x:x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_labels_list=only_labels.collect()\n",
    "print(only_labels.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Artisthotness positive slope\n",
    "#artistfamilarity positive slope\n",
    "#ArtistLatitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#ArtistLongitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#tempo GOOD\n",
    "#KeySignature DISCRETE 0-10\n",
    "#KeySignatureConfidence STILL GOOD\n",
    "#TimeSignature DISCRETE 0-10\n",
    "#TimeSignatureConfidence GOOD 3300\n",
    "#Year GOOD 2700\n",
    "###########Timbre0-11 CAN HAVE NEGATIVE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_features=test_filtered_data.map(lambda x:x.features[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_features_list=only_features.collect()\n",
    "minn=min(only_features_list)\n",
    "maxx=max(only_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(only_features_list[0:100], only_labels_list[0:100], 'ro')\n",
    "plt.axis([minn, maxx, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.hist(only_features_list,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(only_labels_list)/len(only_labels_list))\n",
    "print(len(only_labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.hist(only_labels_list,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=test_filtered_data\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)\n",
    "\n",
    "only_train_labels=from_0_train.map(lambda x:x.label).collect()\n",
    "\n",
    "#print sum(only_train_labels)/len(only_train_labels)\n",
    "mean_train=np.median(only_train_labels)\n",
    "print(len(only_train_labels))\n",
    "\n",
    "def tolabelpoint_binary(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "  fl=0\n",
    "  if line.label>mean_train:\n",
    "    fl=1\n",
    "  else:\n",
    "    fl=0\n",
    "  \n",
    "  return LabeledPoint(fl,line.features)\n",
    "\n",
    "sparkDF_new_f_label_binary=test_filtered_data_df.map(lambda x:tolabelpoint_binary(x))\n",
    "\n",
    "test_filtered_data_df=sparkDF_new_f_label_binary\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from_0_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_train_labels=from_0_train.map(lambda x:x.label).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print sum(only_train_labels)/len(only_train_labels)\n",
    "mean_train=sum(only_train_labels)/len(only_train_labels)\n",
    "print len(only_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tolabelpoint_binary(line):\n",
    "  print line\n",
    "  #return line[1:]\n",
    "  fl=0\n",
    "  if line.label>mean_train:\n",
    "    fl=1\n",
    "  else:\n",
    "    fl=0\n",
    "  \n",
    "  return LabeledPoint(fl,line.features)\n",
    "\n",
    "sparkDF_new_f_label_binary=test_filtered_data_df.map(lambda x:tolabelpoint_binary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f_label_binary.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#L REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=test_filtered_data\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from_0_train.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_train_labels=from_0_train.map(lambda x:x.label).collect()\n",
    "\n",
    "median_train=np.median(only_train_labels)\n",
    "percentile_25=np.percentile(only_train_labels, 25)\n",
    "percentile_75=np.percentile(only_train_labels, 75)\n",
    "mean_train=median_train\n",
    "\n",
    "print percentile_25\n",
    "print median_train\n",
    "print percentile_75\n",
    "\n",
    "mean_train=round(mean_train,2)\n",
    "pred_label=from_0_val.map(lambda x:(round(x.label,2),mean_train))\n",
    "\n",
    "\n",
    "pred_label.collect()\n",
    "predictionAndObservations = sc.parallelize(pred_label.collect())\n",
    "metrics = RegressionMetrics(predictionAndObservations)\n",
    "# metrics.explainedVariance\n",
    "# # 8.859...\n",
    "# # >>> metrics.meanAbsoluteError\n",
    "# # 0.5...\n",
    "# # >>> metrics.meanSquaredError\n",
    "# # 0.37...\n",
    "print \"RMSE before linear regression\"\n",
    "print metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bin_predict(line):\n",
    "  fl=0\n",
    "  if line>mean_train:\n",
    "    fl=1\n",
    "  return fl \n",
    "\n",
    "def fin_acc(l_p):\n",
    "  siz=len(l_p)\n",
    "  match=0\n",
    "  for i in range(0,len(l_p)):\n",
    "    if int(l_p[i][0])==int(l_p[i][1]):\n",
    "      match=match+1\n",
    "  acc=1.0*match/siz\n",
    "  return acc\n",
    "  \n",
    "\n",
    "b=from_0_train.collect()\n",
    "training=b\n",
    "training_df=sqlContext.createDataFrame(training)\n",
    "\n",
    "val_collect=from_0_val.collect()\n",
    "val_collect_df=sqlContext.createDataFrame(val_collect)\n",
    "\n",
    "X = np.arange(0, 1.1, 0.1)\n",
    "Y = np.arange(0, 1.1, 0.1)\n",
    "X_map=np.empty([11,11])\n",
    "Y_map=np.empty([11,11])\n",
    "Z_map=np.empty([11,11])\n",
    "\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "for i in range(0,len(X)):\n",
    "  for j in range(0,len(Y)):\n",
    "    lam=X[i]\n",
    "    alpha=Y[j]\n",
    "    X_map[i][j]=lam\n",
    "    Y_map[i][j]=alpha\n",
    "    \n",
    "    print str(lam)+\" : \"+str(alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lr = LinearRegression(maxIter=100, regParam=lam, elasticNetParam=alpha)\n",
    "\n",
    "    # Fit the model\n",
    "    lrModel = lr.fit(training_df)\n",
    "    \n",
    "    model_result_val=lrModel.transform(val_collect_df)\n",
    "\n",
    "    labelsAndPreds=model_result_val.map(lambda x: (x.label,x.prediction))\n",
    "    \n",
    "    \n",
    "#     print labelsAndPreds\n",
    "#     break\n",
    "    predictionAndObservations = sc.parallelize(labelsAndPreds.collect())\n",
    "    metrics = RegressionMetrics(predictionAndObservations)\n",
    "    print metrics.rootMeanSquaredError\n",
    "    Z_map[i][j]=metrics.rootMeanSquaredError\n",
    "    #break\n",
    "    \n",
    "#     p=\" %\"\n",
    "#     acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "#     print str(acc)+p\n",
    "#     Z_map[i][j]=acc\n",
    "\n",
    "# # Print the coefficients and intercept for linear regression\n",
    "# print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "# print(\"Intercept: \" + str(lrModel.intercept))\n",
    "# #print(\"result: \" + str(lrModel.transform(a)))\n",
    "# display(lrModel.transform(a).collect()[0:15])\n",
    "# train_model_transform=lrModel.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print X_map\n",
    "print Y_map\n",
    "Z_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_indexes=np.where(Z_map==Z_map.min())\n",
    "print min_indexes\n",
    "\n",
    "print Z_map.min()\n",
    "print X_map[min_indexes[0][0]][min_indexes[1][0]]\n",
    "print Y_map[min_indexes[0][0]][min_indexes[1][0]]\n",
    "\n",
    "\n",
    "best_lam=X_map[min_indexes[0][0]][min_indexes[1][0]]\n",
    "best_alpha=Y_map[min_indexes[0][0]][min_indexes[1][0]]\n",
    "# print Z_map_coef[max_indexes[0][0]][max_indexes[1][0]]\n",
    "# print Z_map_inter[max_indexes[0][0]][max_indexes[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100, regParam=best_lam, elasticNetParam=best_alpha)\n",
    "\n",
    "    # Fit the model\n",
    "lrModel = lr.fit(training_df)\n",
    "    \n",
    "model_result_val=lrModel.transform(val_collect_df)\n",
    "\n",
    "labelsAndPreds=model_result_val.map(lambda x: (x.label,x.prediction))\n",
    "predictionAndObservations = sc.parallelize(labelsAndPreds.collect())\n",
    "metrics = RegressionMetrics(predictionAndObservations)\n",
    "print metrics.rootMeanSquaredError\n",
    "\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "    #co1=str(lrModel.coefficients)\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "    #in1=str(lrModel.intercept)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_collect=from_0_test.collect()\n",
    "# test_collect_df=sqlContext.createDataFrame(test_collect)\n",
    "# model_result_val=lrModel.transform(test_collect_df)\n",
    "\n",
    "# labelsAndPreds=model_result_val.map(lambda x: (x.label,make_bin_predict(x.prediction)))\n",
    "# p=\" %\"\n",
    "# acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "# print \"Test Set\"\n",
    "# print str(acc)+p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# imports specific to the plots in this example\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d.axes3d import get_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X=X_map\n",
    "Y=Y_map\n",
    "Z=Z_map\n",
    "plt.clf()\n",
    "# Twice as wide as it is tall.\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "\n",
    "#---- First subplot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "# X = np.arange(0, 1, 0.2)\n",
    "# Y = np.arange(0, 1, 0.2)\n",
    "# X, Y = np.meshgrid(X, Y)\n",
    "# R = np.sqrt(X**2 + Y**2)\n",
    "# Z =R\n",
    "# Z=np.array([[1,2,3,4,5],[1,1,1,1,1],[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3]])\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.set_xlabel('Lambda Reg Param')\n",
    "ax.set_ylabel('Alpha L1 L2 tradeoff')\n",
    "ax.set_zlim3d(0, 0.2)\n",
    "for angle in range(60, 300):\n",
    "    ax.view_init(18, angle)\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "#---- Second subplot\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "#X, Y, Z = get_test_data(0.05)\n",
    "ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n",
    "\n",
    "ax.set_xlabel('Lambda Reg Param')\n",
    "ax.set_ylabel('Alpha L1 L2 tradeoff')\n",
    "\n",
    "ax.set_zlim3d(0, 0.2)\n",
    "for angle in range(60, 300):\n",
    "    ax.view_init(18, angle)\n",
    "    \n",
    "\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_collect=from_0_val.collect()\n",
    "# val_collect_df=sqlContext.createDataFrame(val_collect)\n",
    "\n",
    "\n",
    "# model_result=lrModel.transform(val_collect_df)\n",
    "# display(model_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def make_bin_predict(line):\n",
    "#   fl=0\n",
    "#   if line>0.45:\n",
    "#     fl=1\n",
    "#   return fl  \n",
    "  \n",
    "# labelsAndPreds=model_result.map(lambda x: (x.label,make_bin_predict(x.prediction)))\n",
    "# display(labelsAndPreds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fin_acc(l_p):\n",
    "#   siz=len(l_p)\n",
    "#   match=0\n",
    "#   for i in range(0,len(l_p)):\n",
    "#     if int(l_p[i][0])==int(l_p[i][1]):\n",
    "#       match=match+1\n",
    "#   acc=1.0*match/siz\n",
    "#   return acc\n",
    "  \n",
    "# p=\" %\"\n",
    "# acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "# print str(acc)+p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## LOGISTIC START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=test_filtered_data\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)\n",
    "\n",
    "only_train_labels=from_0_train.map(lambda x:x.label).collect()\n",
    "\n",
    "#print sum(only_train_labels)/len(only_train_labels)\n",
    "mean_train=np.median(only_train_labels)\n",
    "print len(only_train_labels)\n",
    "\n",
    "def tolabelpoint_binary(line):\n",
    "  print line\n",
    "  #return line[1:]\n",
    "  fl=0\n",
    "  if line.label>mean_train:\n",
    "    fl=1\n",
    "  else:\n",
    "    fl=0\n",
    "  \n",
    "  return LabeledPoint(fl,line.features)\n",
    "\n",
    "sparkDF_new_f_label_binary=test_filtered_data_df.map(lambda x:tolabelpoint_binary(x))\n",
    "\n",
    "test_filtered_data_df=sparkDF_new_f_label_binary\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fin_acc(l_p):\n",
    "  siz=len(l_p)\n",
    "  match=0\n",
    "  for i in range(0,len(l_p)):\n",
    "    if int(l_p[i][0])==int(l_p[i][1]):\n",
    "      match=match+1\n",
    "  acc=1.0*match/siz\n",
    "  return acc\n",
    "  \n",
    "\n",
    "b=from_0_train.collect()\n",
    "training=b\n",
    "training_df=sqlContext.createDataFrame(training)\n",
    "\n",
    "val_collect=from_0_val.collect()\n",
    "val_collect_df=sqlContext.createDataFrame(val_collect)\n",
    "\n",
    "X = np.arange(0, 1.1, 0.1)\n",
    "Y = np.arange(0, 1.1, 0.1)\n",
    "X_map=np.empty([11,11])\n",
    "Y_map=np.empty([11,11])\n",
    "Z_map=np.empty([11,11])\n",
    "\n",
    "# Z_map_coef=np.chararray((11, 11),itemsize=100)\n",
    "# Z_map_inter=np.chararray((11, 11),itemsize=100)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "for i in range(0,len(X)):\n",
    "  for j in range(0,len(Y)):\n",
    "  \n",
    "    \n",
    "    lam=X[i]\n",
    "    alpha=Y[j]\n",
    "    X_map[i][j]=lam\n",
    "    Y_map[i][j]=alpha\n",
    "\n",
    "\n",
    "# Load training data\n",
    "#training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print str(lam)+\" : \"+str(alpha)\n",
    "    lr2 = LogisticRegression(maxIter=100, regParam=lam, elasticNetParam=alpha)\n",
    "\n",
    "    # Fit the model\n",
    "    lrModel2 = lr2.fit(training_df)\n",
    "\n",
    "    model2_result_val=lrModel2.transform(val_collect_df)\n",
    "\n",
    "    labelsAndPreds=model2_result_val.map(lambda x: (x.label,x.prediction))\n",
    "    p=\" %\"\n",
    "    acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "    print str(acc)+p\n",
    "    Z_map[i][j]=acc\n",
    "#     Z_map_coef[i][j]=str(lrModel.coefficients)\n",
    "#     Z_map_inter[i][j]=str(lrModel.intercept)\n",
    "    \n",
    "    print(\"Coefficients: \" + str(lrModel2.coefficients))\n",
    "    print(\"Intercept: \" + str(lrModel2.intercept))\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_indexes=np.where(Z_map==Z_map.max())\n",
    "print max_indexes\n",
    "\n",
    "print Z_map.max()\n",
    "print X_map[max_indexes[0][0]][max_indexes[1][0]]\n",
    "print Y_map[max_indexes[0][0]][max_indexes[1][0]]\n",
    "\n",
    "best_lam=X_map[max_indexes[0][0]][max_indexes[1][0]]\n",
    "best_alpha=Y_map[max_indexes[0][0]][max_indexes[1][0]]\n",
    "# print Z_map_coef[max_indexes[0][0]][max_indexes[1][0]]\n",
    "# print Z_map_inter[max_indexes[0][0]][max_indexes[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(maxIter=100, regParam=best_lam, elasticNetParam=best_alpha)\n",
    "\n",
    "    # Fit the model\n",
    "lrModel2 = lr2.fit(training_df)\n",
    "    \n",
    "model_result_val=lrModel2.transform(val_collect_df)\n",
    "\n",
    "labelsAndPreds=model_result_val.map(lambda x: (x.label,x.prediction))\n",
    "p=\" %\"\n",
    "acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "print str(acc)+p\n",
    "print(\"Coefficients: \" + str(lrModel2.coefficients))\n",
    "    #co1=str(lrModel.coefficients)\n",
    "print(\"Intercept: \" + str(lrModel2.intercept))\n",
    "    #in1=str(lrModel.intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_collect=from_0_test.collect()\n",
    "test_collect_df=sqlContext.createDataFrame(test_collect)\n",
    "model_result_val=lrModel2.transform(test_collect_df)\n",
    "\n",
    "labelsAndPreds=model_result_val.map(lambda x: (x.label,make_bin_predict(x.prediction)))\n",
    "p=\" %\"\n",
    "acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "print \"Test Set\"\n",
    "print str(acc)+p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X=X_map\n",
    "Y=Y_map\n",
    "Z=Z_map\n",
    "plt.clf()\n",
    "# Twice as wide as it is tall.\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "\n",
    "#---- First subplot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "# X = np.arange(0, 1, 0.2)\n",
    "# Y = np.arange(0, 1, 0.2)\n",
    "# X, Y = np.meshgrid(X, Y)\n",
    "# R = np.sqrt(X**2 + Y**2)\n",
    "# Z =R\n",
    "# Z=np.array([[1,2,3,4,5],[1,1,1,1,1],[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3]])\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ax.set_xlabel('Lambda Reg Param')\n",
    "ax.set_ylabel('Alpha L1 L2 tradeoff')\n",
    "ax.set_zlim3d(30, 100)\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "\n",
    "#---- Second subplot\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "#X, Y, Z = get_test_data(0.05)\n",
    "ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n",
    "\n",
    "ax.set_xlabel('Lambda Reg Param')\n",
    "ax.set_ylabel('Alpha L1 L2 tradeoff')\n",
    "\n",
    "ax.set_zlim3d(30, 100)\n",
    "for angle in range(150, 3300):\n",
    "    ax.view_init(20, angle)\n",
    "    \n",
    "\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labelsAndPreds=model_result.map(lambda x: (x.label,x.prediction))\n",
    "# display(labelsAndPreds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fin_acc(l_p):\n",
    "#   siz=len(l_p)\n",
    "#   match=0\n",
    "#   for i in range(0,len(l_p)):\n",
    "#     if int(l_p[i][0])==int(l_p[i][1]):\n",
    "#       match=match+1\n",
    "#   acc=1.0*match/siz\n",
    "#   return acc\n",
    "  \n",
    "# p=\" %\"\n",
    "# acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "# print str(acc)+p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  },
  "name": "million_songs_biggest_for_all_norm",
  "notebookId": 2620325829841088
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
