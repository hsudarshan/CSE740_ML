{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PLOT test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.plot([300.0,301.0,302.0,303.0], [1,7,12,16], 'ro')\n",
    "plt.axis([290, 310, 0, 20])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%fs ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "#C folder is uzzxeqtc1480659570445 working fine tested\n",
    "#B folder is i9k46ctm1480700408609\n",
    "#A folder is unfd0khf1480700528583\n",
    "\n",
    "\n",
    "sparkDF = sqlContext.read.format(\"com.databricks.spark.csv\").load(\"gs://millionsongsoutput/dataABC/*.csv\",header='true')\n",
    "\n",
    "#sparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/unfd0khf1480700528583/\",header='true')#A\n",
    "\n",
    "# sparkDF2 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/i9k46ctm1480700408609/\",header='true')#B\n",
    "# sparkDF3 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/uzzxeqtc1480659570445/\",header='true')#C\n",
    "# sparkDF4 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/zh8w7cvo1481174995446/\",header='true')#D\n",
    "# sparkDF5 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/cmcoiahn1481175345159/\",header='true')#E\n",
    "# sparkDF6 = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/uu0j5t8x1481175546319/\",header='true')#F\n",
    "\n",
    "\n",
    "\n",
    "# appended = sparkDF1.unionAll(sparkDF2)\n",
    "\n",
    "# appended = appended.unionAll(sparkDF3)\n",
    "# appended = appended.unionAll(sparkDF4)\n",
    "# appended = appended.unionAll(sparkDF5)\n",
    "# sparkDF=appended.unionAll(sparkDF3)\n",
    "\n",
    "print(sparkDF.count())\n",
    "print(sparkDF.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "sparkDF.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "#x[\"Danceability\"],x[\"ArtistLatitude\"],x[\"ArtistLongitude\"],x[\"SongNumber\"]\n",
    "sparkDF_new=sparkDF.map(lambda x:[x[\"Year\"],x[\"song_hotttnesss\"],x[\"artist_hotttnesss\"],x[\"artist_familiarity\"],x[\"Tempo\"],x[\"KeySignature\"],x[\"KeySignatureConfidence\"],x[\"TimeSignature\"],x[\"TimeSignatureConfidence\"],x[\"timbre0\"],x[\"timbre1\"],x[\"timbre2\"],x[\"timbre3\"],x[\"timbre4\"],x[\"timbre5\"],x[\"timbre6\"],x[\"timbre7\"],x[\"timbre8\"],x[\"timbre9\"],x[\"timbre10\"],x[\"timbre11\"],x[\"Duration\"]])\n",
    "\n",
    "\n",
    "#,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(sparkDF_new))\n",
    "sparkDF_new.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new_filter_bf=sparkDF_new.filter(lambda x:x[0]!='nan' and x[1]!='null' and x[2]!='null' and x[3]!='null' and x[4]!='null' and x[5]!='null' and x[6]!='null' and x[7]!='null' and x[8]!='null' and x[9]!='null' and x[10]!='null' and x[11]!='null' and x[12]!='null' and x[13]!='null' and x[14]!='null' and x[15]!='null' and x[16]!='null' and x[17]!='null' and x[18]!='null' and x[19]!='null' and x[20]!='null' and x[21]!='null')\n",
    "\n",
    "\n",
    "sparkDF_new_filter=sparkDF_new_filter_bf.filter(lambda x:x[0]!='nan' and x[1]!='nan' and x[2]!='nan' and x[3]!='nan' and x[4]!='nan' and x[5]!='nan' and x[6]!='nan' and x[7]!='nan' and x[8]!='nan' and x[9]!='nan' and x[10]!='nan' and x[11]!='nan' and x[12]!='nan' and x[13]!='nan' and x[14]!='nan' and x[15]!='nan' and x[16]!='nan' and x[17]!='nan' and x[18]!='nan' and x[19]!='nan' and x[20]!='nan' and x[21]!='nan')\n",
    "# \n",
    "#NaN \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sparkDF_new_filter.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHECK FOR EMPTY COLUMNS\n",
    "\n",
    "#Artisthotness GOOD\n",
    "#artistfamilarity GOOD\n",
    "#ArtistLatitude 2196 remains\n",
    "#ArtistLongitude 2196 remains\n",
    "#tempo GOOD\n",
    "#KeySignature GOOD\n",
    "#KeySignatureConfidence GOOD\n",
    "#TimeSignature GOOD\n",
    "#TimeSignatureConfidence GOOD\n",
    "#Year GOOD\n",
    "#Timbre0-11 GOOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty_filter_test=sparkDF_new_filter.filter(lambda x:x[0]!=\"\" and x[1]!=\"\" and x[2]!=\"\" and x[3]!=\"\" and x[4]!=\"\" and x[5]!=\"\" and x[6]!=\"\" and x[7]!=\"\" and x[8]!=\"\" and x[9]!=\"\" and x[10]!=\"\" and x[11]!=\"\" and x[12]!=\"\" and x[13]!=\"\" and x[14]!=\"\" and x[15]!=\"\" and x[16]!=\"\" and x[17]!=\"\" and x[18]!=\"\" and x[19]!=\"\" and x[20]!=\"\" and x[21]!=\"\")\n",
    "empty_filter_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty_filter_test.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f=empty_filter_test.map(lambda x:[float(x[0]),float(x[1]),float(x[2]),float(x[3]),float(x[4]),float(x[5]),float(x[6]),float(x[7]),float(x[8]),float(x[9]),float(x[10]),float(x[11]),float(x[12]),float(x[13]),float(x[14]),float(x[15]),float(x[16]),float(x[17]),float(x[18]),float(x[19]),float(x[20]),float(x[21])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f=sparkDF_new_f.filter(lambda x:x[0]>0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXPANSION \n",
    "\n",
    "#sparkDF_new_f=sparkDF_new_f.map(lambda x:[round(x[0],2),round(x[1],2),round(x[1]*x[1],2),round(x[1]*x[1]*x[1],2),round(x[2],2),round(x[2]*x[2],2),round(x[2]*x[2]*x[2],2)])\n",
    "\n",
    "#sparkDF_new_f=sparkDF_new_f.map(lambda x:[round(x[0],2),round(x[1],2),round(x[2],2)])\n",
    "\n",
    "# sparkDF_new_f=sparkDF_new_f.map(lambda x:[round(x[0],2),round(x[1],2),round(x[1]*x[1],2),round(x[1]*x[1]*x[1],2),round(x[2],2),round(x[2]*x[2],2),round(x[2]*x[2]*x[2],2),float(x[3]),float(x[4]),float(x[5]),float(x[6]),float(x[7]),float(x[8]),float(x[9]),float(x[10]),float(x[11]),float(x[12]),float(x[13]),float(x[14]),float(x[15]),float(x[16]),float(x[17]),float(x[18]),float(x[19]),float(x[20]),float(x[21])])\n",
    "\n",
    "\n",
    "#sparkDF_new_f=sparkDF_new_f.map(lambda x:[x[0],x[1],x[2]*x[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f_above_0_hot=sparkDF_new_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sparkDF_new_f_above_0_hot_df=sqlContext.createDataFrame(sparkDF_new_f_above_0_hot)\n",
    "sparkDF_new_f_above_0_hot_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_grouped_by_year=sparkDF_new_f_above_0_hot_df.filter(sparkDF_new_f_above_0_hot_df._1>1986.0).groupBy(['_1', sparkDF_new_f_above_0_hot_df._1]).mean().sort('_1', ascending=True) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_grouped_by_year.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_labels_list=mean_grouped_by_year.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##'avg(_2)' is the first feature\n",
    "\n",
    "\n",
    "only_features_list=mean_grouped_by_year.map(lambda x:x['avg(_5)']).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "minn=min(only_features_list)\n",
    "maxx=max(only_features_list)\n",
    "\n",
    "minn_label=min(only_labels_list)\n",
    "maxx_label=max(only_labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x[\"song_hotttnesss\"],x[\"artist_hotttnesss\"],x[\"artist_familiarity\"],x[\"Tempo\"],x[\"KeySignature\"],x[\"KeySignatureConfidence\"],x[\"TimeSignature\"],x[\"TimeSignatureConfidence\"],x[\"timbre0\"],x[\"timbre1\"],x[\"timbre2\"],x[\"timbre3\"],x[\"timbre4\"],x[\"timbre5\"],x[\"timbre6\"],x[\"timbre7\"],x[\"timbre8\"],x[\"timbre9\"],x[\"timbre10\"],x[\"timbre11\"],x[\"Duration\"]])\n",
    "\n",
    "\n",
    "#song hotness increasing average\n",
    "#artist hotness somewhat increasing\n",
    "#artish familarity also increasing\n",
    "#Tempo stable \n",
    "#KeySignature stabel slight decrease\n",
    "#KeySignatureConfidence decreasing\n",
    "#TimeSignature slight increase\n",
    "#TimeSignatureConfidence slight increase\n",
    "#timbre 0 increase\n",
    "#timbre 1 decrease\n",
    "#timbre 2 somedecrease\n",
    "#timbre 3 stable\n",
    "#timbre 4 slight increase\n",
    "#timbre 5 decrease\n",
    "#timbre 6 increase\n",
    "\n",
    "#timbre 7 stable zigzag\n",
    "\n",
    "#timbre 8 increase\n",
    "\n",
    "#timbre 9 stable zigzag\n",
    "#timbre 10 stable\n",
    "#timbre 11 stable\n",
    "\n",
    "\n",
    "#Duration stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minn_label, maxx_label,minn, maxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot( only_labels_list,only_features_list, 'ro')\n",
    "plt.axis([ minn_label, maxx_label,minn, maxx])\n",
    "#plt.axis([ minn_label, maxx_label,0.382, 0.545])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHECK FOR 0 COLUMNS\n",
    "\n",
    "#Artisthotness GOOD 4200\n",
    "#artistfamilarity GOOD\n",
    "#ArtistLatitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#ArtistLongitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#tempo GOOD\n",
    "#KeySignature STILL GOOD\n",
    "#KeySignatureConfidence STILL GOOD\n",
    "#TimeSignature STILL GOOD\n",
    "#TimeSignatureConfidence GOOD 3300\n",
    "#Year NOT GOOD 2700 we need to ignore 0 if we need to proceed\n",
    "###########Timbre0-11 CAN HAVE NEGATIVE VALUES\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# [x[\"song_hotttnesss\"],x[\"artist_hotttnesss\"],x[\"artist_familiarity\"],x[\"Tempo\"],x[\"KeySignature\"],x[\"KeySignatureConfidence\"],x[\"TimeSignature\"],x[\"TimeSignatureConfidence\"],x[\"Year\"],x[\"timbre0\"],x[\"timbre1\"],x[\"timbre2\"],x[\"timbre3\"],x[\"timbre4\"],x[\"timbre5\"],x[\"timbre6\"],x[\"timbre7\"],x[\"timbre8\"],x[\"timbre9\"],x[\"timbre10\"],x[\"timbre11\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sparkDF_new_f_above_0_hot=sparkDF_new_f.filter(lambda x:x[0]>0.0 and x[8]>0.0)\n",
    "\n",
    "# sparkDF_new_f_above_0_hot=sparkDF_new_f.filter(lambda x:x[0]>0.0)\n",
    "\n",
    "# sparkDF_new_f_above_0_hot.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparkDF_new_f=sparkDF_new_filter.map(lambda x:[float(x[0]),float(x[1]),float(x[2]),float(x[3]),float(x[4]),float(x[5]),float(x[6]),float(x[7]),float(x[8]),float(x[9]),float(x[10]),float(x[11]),float(x[12]),float(x[13]),float(x[14])])\n",
    "\n",
    "\n",
    "#,float(x[15]),float(x[16]),float(x[17]),float(x[18]),float(x[19]),float(x[20]),float(x[21]),float(x[22])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(sparkDF_new_f.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "def tolabelpoint(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "    return LabeledPoint(line[0],line[1:])\n",
    "\n",
    "sparkDF_new_f_label=sparkDF_new_f_above_0_hot.map(lambda x:tolabelpoint(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f_label.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filter_df=sqlContext.createDataFrame(sparkDF_new_f_label)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_filter_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_filter_df.groupBy(['label', test_filter_df.label]).count().sort('label', ascending=True).collect()\n",
    "#test_filter_df.groupBy(['label', test_filter_df.features]).mean().sort('label', ascending=True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filter_df_above_100=test_filter_df.groupBy(['label', test_filter_df.label]).count().filter(\"`count` >= 454\").sort(\"label\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_filter_df_above_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sparkDF_new_f_label.collect()\n",
    "#test_filter_df=sqlContext.createDataFrame(sparkDF_new_f_label)\n",
    "#test_filter_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_filter_above_100s = test_filter_df[test_filter_df.label>=1986.0]\n",
    "#test_filter_above_100s=test_filter_df.filter(label>=1986.0 and label<2010.0)\n",
    "test_filter_above_100s=sparkDF_new_f_label.filter(lambda x:x.label>=1986.0 and x.label<2010.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_filter_above_100s.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_filter_above_100s.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkDF_new_f_label=test_filter_above_100s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data=sparkDF_new_f_label.filter(lambda x:x.label>0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_filtered_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=sqlContext.createDataFrame(test_filtered_data.collect())\n",
    "\n",
    "test_filtered_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_labels=test_filtered_data.map(lambda x:x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_labels_list=only_labels.collect()\n",
    "\n",
    "minn_label=min(only_labels_list)\n",
    "maxx_label=max(only_labels_list)\n",
    "\n",
    "print(only_labels.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Artisthotness positive slope\n",
    "#artistfamilarity positive slope\n",
    "#ArtistLatitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#ArtistLongitude 2196 ALREADY REMOVED BECAUSE OF EMPTY\n",
    "#tempo GOOD\n",
    "#KeySignature DISCRETE 0-10\n",
    "#KeySignatureConfidence STILL GOOD\n",
    "#TimeSignature DISCRETE 0-10\n",
    "#TimeSignatureConfidence GOOD 3300\n",
    "#Year GOOD 2700\n",
    "###########Timbre0-11 CAN HAVE NEGATIVE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_features=test_filtered_data.map(lambda x:x.features[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "only_features_list=only_features.collect()\n",
    "minn=min(only_features_list)\n",
    "maxx=max(only_features_list)\n",
    "print(minn)\n",
    "print(maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# :[x[\"song_hotttnesss\"],x[\"artist_hotttnesss\"],x[\"artist_familiarity\"],x[\"Tempo\"],x[\"KeySignature\"],x[\"KeySignatureConfidence\"],x[\"TimeSignature\"],x[\"TimeSignatureConfidence\"],x[\"Year\"],x[\"timbre0\"],x[\"timbre1\"],x[\"timbre2\"],x[\"timbre3\"],x[\"timbre4\"],x[\"timbre5\"],x[\"timbre6\"],x[\"timbre7\"],x[\"timbre8\"],x[\"timbre9\"],x[\"timbre10\"],x[\"timbre11\"]]),x[\"Danceability\"],x[\"Duration\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot( only_labels_list[0:500],only_features_list[0:500], 'ro')\n",
    "plt.axis([ minn_label, maxx_label,minn, maxx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.hist(only_labels_list,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sum(only_labels_list)/len(only_labels_list))\n",
    "print(len(only_labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.hist(only_labels_list,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=test_filtered_data\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 48\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_filtered_data_df.count())\n",
    "print(from_0_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from_0_train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_train_labels=from_0_train.map(lambda x:x.label).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print sum(only_train_labels)/len(only_train_labels)\n",
    "# mean_train=sum(only_train_labels)/len(only_train_labels)\n",
    "# print len(only_train_labels)\n",
    "# print mean_train\n",
    "\n",
    "median_train=np.median(only_train_labels)\n",
    "percentile_25=np.percentile(only_train_labels, 25)\n",
    "percentile_75=np.percentile(only_train_labels, 75)\n",
    "mean_train=median_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(percentile_25)\n",
    "print(median_train)\n",
    "print(percentile_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 25-75 split\n",
    "\n",
    "# from_0_train_filter=from_0_train.filter(lambda x:x.label>=percentile_75 or x.label<=percentile_25)\n",
    "# print from_0_train_filter.count()\n",
    "# from_0_train=from_0_train_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# only_labels_list=from_0_train_filter.map(lambda x:x.label).collect()\n",
    "# plt.clf()\n",
    "# plt.hist(only_labels_list,bins=100)\n",
    "# display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "predictionAndObservations = sc.parallelize([(2.5, 3.0), (0.0, -0.5), (2.0, 2.0), (8.0, 7.0)])\n",
    "metrics = RegressionMetrics(predictionAndObservations)\n",
    "metrics.explainedVariance\n",
    "# 8.859...\n",
    "# >>> metrics.meanAbsoluteError\n",
    "# 0.5...\n",
    "# >>> metrics.meanSquaredError\n",
    "# 0.37...\n",
    "print(metrics.rootMeanSquaredError)\n",
    "# 0.61...\n",
    "# >>> metrics.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_train=round(mean_train,2)\n",
    "pred_label=from_0_val.map(lambda x:(round(x.label,2),mean_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred_label.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_label.collect()\n",
    "predictionAndObservations = sc.parallelize(pred_label.collect())\n",
    "metrics = RegressionMetrics(predictionAndObservations)\n",
    "# metrics.explainedVariance\n",
    "# # 8.859...\n",
    "# # >>> metrics.meanAbsoluteError\n",
    "# # 0.5...\n",
    "# # >>> metrics.meanSquaredError\n",
    "# # 0.37...\n",
    "#print(\"RMSE before linear regression\")\n",
    "#print(metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Values to use when training the linear regression model\n",
    "\n",
    "# num_iters = 100  # iterations\n",
    "# reg = 1e-1  # regParam\n",
    "# alpha = .2  # elasticNetParam\n",
    "# use_intercept = True  # intercept\n",
    "\n",
    "\n",
    "b=from_0_train.collect()\n",
    "\n",
    "\n",
    "\n",
    "# print b\n",
    "# print stop\n",
    "c=[LabeledPoint(1.0, [0.574274730517]), LabeledPoint(1.0, [0.401997543364]), LabeledPoint(1.0, [0.401723685504]), LabeledPoint(0.0, [0.332275746599]), LabeledPoint(0.0, [0.351555861186]), LabeledPoint(0.0, [0.44793548048]), LabeledPoint(0.0, [0.330806356856]), LabeledPoint(1.0, [0.513463289702]), LabeledPoint(0.0, [0.37825468551]), LabeledPoint(1.0, [0.541888972034]), LabeledPoint(1.0, [0.306242264246]), LabeledPoint(0.0, [0.363651365059]), LabeledPoint(0.0, [0.416173064627]), LabeledPoint(0.0, [0.452789476752])]\n",
    "#b=c\n",
    "a=sqlContext.createDataFrame(b)\n",
    "print(type(b))\n",
    "#display(a)\n",
    "\n",
    "lr = LinearRegression(maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(a)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "#print(\"result: \" + str(lrModel.transform(a)))\n",
    "lrModel.transform(a).collect()[0:15]\n",
    "train_model_transform=lrModel.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#song hotness increasing average\n",
    "#artist hotness somewhat increasing\n",
    "#artish familarity also increasing\n",
    "#Tempo stable \n",
    "#KeySignature stabel slight decrease\n",
    "#KeySignatureConfidence decreasing\n",
    "#TimeSignature slight increase\n",
    "#TimeSignatureConfidence slight increase\n",
    "#timbre 0 increase\n",
    "#timbre 1 decrease\n",
    "#timbre 2 somedecrease\n",
    "#timbre 3 stable\n",
    "#timbre 4 slight increase\n",
    "#timbre 5 decrease\n",
    "#timbre 6 increase\n",
    "\n",
    "#timbre 7 stable zigzag\n",
    "\n",
    "#timbre 8 increase\n",
    "\n",
    "#timbre 9 stable zigzag\n",
    "#timbre 10 stable\n",
    "#timbre 11 stable\n",
    "\n",
    "\n",
    "#Duration stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####COEFFCIENTS with normalization\n",
    "\n",
    "\n",
    "\n",
    "#Artisthotness .4\n",
    "#artistfamilarity 1.1\n",
    "#tempo 0.0001\n",
    "#KeySignature 0.0002\n",
    "#KeySignatureConfidence 0.04\n",
    "#TimeSignature 0.008\n",
    "#TimeSignatureConfidence 0.013\n",
    "#Year 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_collect=from_0_val.collect()\n",
    "val_collect_df=sqlContext.createDataFrame(val_collect)\n",
    "\n",
    "\n",
    "model_result=lrModel.transform(val_collect_df)\n",
    "model_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_bin_predict(line):\n",
    "    fl=0\n",
    "    if line>mean_train:\n",
    "        fl=1\n",
    "    return fl  \n",
    "  \n",
    "pred_label=model_result.map(lambda x: (round(x.label,2),round(x.prediction,2)))\n",
    "\n",
    "\n",
    "predictionAndObservations = sc.parallelize(pred_label.collect())\n",
    "metrics = RegressionMetrics(predictionAndObservations)\n",
    "# metrics.explainedVariance\n",
    "# # 8.859...\n",
    "# # >>> metrics.meanAbsoluteError\n",
    "# # 0.5...\n",
    "# # >>> metrics.meanSquaredError\n",
    "# # 0.37...\n",
    "print(\"RMSE after linear regression\")\n",
    "print(metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fin_acc(l_p):\n",
    "    siz=len(l_p)\n",
    "    match=0\n",
    "    for i in range(0,len(l_p)):\n",
    "        if int(l_p[i][0])==int(l_p[i][1]):\n",
    "            match=match+1\n",
    "    acc=1.0*match/siz\n",
    "    return acc\n",
    "  \n",
    "# p=\" %\"\n",
    "# acc=round(fin_acc(labelsAndPreds.collect())*100,2)\n",
    "# print str(acc)+p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without norm 66.67 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_filtered_data_df=test_filtered_data\n",
    "\n",
    "# weights = [.8, .1, .1]\n",
    "# seed = 48\n",
    "# from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_filtered_data_df.take(5)\n",
    "test_filtered_data_df=sqlContext.createDataFrame(test_filtered_data.collect())\n",
    "test_filtered_data_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(test_filtered_data_df)\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "#lInfNormData = normalizer.transform(test_filtered_data_df, {normalizer.p: float(\"inf\")}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1NormData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tolabelpoint_fromnorm(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "    return LabeledPoint(line.label,line.normFeatures)\n",
    "\n",
    "l1NormData_labelpointed=l1NormData.map(lambda x:tolabelpoint_fromnorm(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1NormData_labelpointed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=l1NormData_labelpointed\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 48\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_filtered_data_df.count())\n",
    "print(from_0_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=test_filtered_data\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 48\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_filtered_data_df.count())\n",
    "print(from_0_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LINEAR after NORM\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Values to use when training the linear regression model\n",
    "\n",
    "# num_iters = 100  # iterations\n",
    "# reg = 1e-1  # regParam\n",
    "# alpha = .2  # elasticNetParam\n",
    "# use_intercept = True  # intercept\n",
    "\n",
    "\n",
    "b=from_0_train.collect()\n",
    "\n",
    "\n",
    "\n",
    "# print b\n",
    "# print stop\n",
    "c=[LabeledPoint(1.0, [0.574274730517]), LabeledPoint(1.0, [0.401997543364]), LabeledPoint(1.0, [0.401723685504]), LabeledPoint(0.0, [0.332275746599]), LabeledPoint(0.0, [0.351555861186]), LabeledPoint(0.0, [0.44793548048]), LabeledPoint(0.0, [0.330806356856]), LabeledPoint(1.0, [0.513463289702]), LabeledPoint(0.0, [0.37825468551]), LabeledPoint(1.0, [0.541888972034]), LabeledPoint(1.0, [0.306242264246]), LabeledPoint(0.0, [0.363651365059]), LabeledPoint(0.0, [0.416173064627]), LabeledPoint(0.0, [0.452789476752])]\n",
    "#b=c\n",
    "a=sqlContext.createDataFrame(b)\n",
    "print(type(b))\n",
    "#display(a)\n",
    "\n",
    "lr = LinearRegression(maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(a)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "#print(\"result: \" + str(lrModel.transform(a)))\n",
    "lrModel.transform(a).collect()[0:15]\n",
    "train_model_transform=lrModel.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_collect=from_0_val.collect()\n",
    "val_collect_df=sqlContext.createDataFrame(val_collect)\n",
    "\n",
    "\n",
    "model_result=lrModel.transform(val_collect_df)\n",
    "model_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_bin_predict(line):\n",
    "    fl=0\n",
    "    if line>mean_train:\n",
    "        fl=1\n",
    "    return fl  \n",
    "  \n",
    "pred_label=model_result.map(lambda x: (round(x.label,2),round(x.prediction,2)))\n",
    "\n",
    "\n",
    "predictionAndObservations = sc.parallelize(pred_label.collect())\n",
    "metrics = RegressionMetrics(predictionAndObservations)\n",
    "# metrics.explainedVariance\n",
    "# # 8.859...\n",
    "# # >>> metrics.meanAbsoluteError\n",
    "# # 0.5...\n",
    "# # >>> metrics.meanSquaredError\n",
    "# # 0.37...\n",
    "print(\"RMSE after linear regression\")\n",
    "print(metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### year from 1 to N\n",
    "#l1NormData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_labels=test_filtered_data.map(lambda x:x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1NormData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meadian_l=np.median(l1NormData.map(lambda x:x.label).collect())\n",
    "meadian_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minn_label\n",
    "minn_label_minus_1=minn_label-1\n",
    "\n",
    "\n",
    "def to_label_from_1(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "    return LabeledPoint(line.label-minn_label_minus_1,line.normFeatures)\n",
    "\n",
    "\n",
    "def to_label_from_1990and200(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "    after=0\n",
    "    if(line.label<2000.0 and line.label>1990):\n",
    "        after=0\n",
    "    elif(line.label<2010.0 and line.label>2000):\n",
    "        after=1\n",
    "    else:\n",
    "        after=2\n",
    "    return LabeledPoint(after,line.normFeatures)\n",
    "\n",
    "def to_label_from_mean(line):\n",
    "  #print line\n",
    "  #return line[1:]\n",
    "    after=0\n",
    "    if(line.label<meadian_l):\n",
    "        after=0\n",
    "    else:\n",
    "        after=1 \n",
    "    return LabeledPoint(after,line.features)\n",
    "\n",
    "\n",
    "l1NormData_yr_from_1_labelpointed=l1NormData.map(lambda x:to_label_from_1990and200(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1NormData_yr_from_1_labelpointed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=l1NormData_yr_from_1_labelpointed\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 48\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## RANDOM START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filtered_data_df=l1NormData_yr_from_1_labelpointed\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 48\n",
    "from_0_train, from_0_val, from_0_test = test_filtered_data_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights = [.8, .1, .1]\n",
    "# #seed = 42\n",
    "# from_0_train, from_0_val, from_0_test = sparkDF_new_f_label_binary.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tolabelpoint_binary(line):\n",
    "    print(line)\n",
    "  #return line[1:\n",
    "    fl=0\n",
    "    if line.label>mean_train:\n",
    "        fl=1\n",
    "    else:\n",
    "        fl=0\n",
    "    return LabeledPoint(fl,line.features)\n",
    "\n",
    "\n",
    "#sparkDF_new_f_label_binary=test_filtered_data_df.map(lambda x:tolabelpoint_binary(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "only_labels_list=from_0_train.map(lambda x:x.label).collect()\n",
    "plt.clf()\n",
    "plt.hist(only_labels_list,bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from_0_train=from_0_train.map(lambda x:tolabelpoint_binary(x))\n",
    "# from_0_val=from_0_val.map(lambda x:tolabelpoint_binary(x))\n",
    "# from_0_test=from_0_test.map(lambda x:tolabelpoint_binary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from_0_train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from_0_train_df=sqlContext.createDataFrame(from_0_train.collect())\n",
    "from_0_val_df=sqlContext.createDataFrame(from_0_val.collect())\n",
    "#from_0_train_df.collect()\n",
    "\n",
    "from_0_train_df.groupBy(['label', from_0_train_df.label]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(from_0_train_df.groupBy(\"label\").count().orderBy(\"label\").show())\n",
    "print(from_0_val_df.groupBy(\"label\").count().orderBy(\"label\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(2406.0/6108)\n",
    "print(307.0/764)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from_0_train_df_0=from_0_train_df.filter(from_0_train_df.label==0.0).takeSample(False, 2406, seed=0)\n",
    "sampled_train = from_0_train_df.sampleBy(\"label\", fractions={0: 1.0, 1: 0.393}, seed=0)\n",
    "sampled_val = from_0_val_df.sampleBy(\"label\", fractions={0: 1.0, 1: 0.401}, seed=0)\n",
    "#from_0_train_df_1=from_0_train_df.filter(from_0_train_df.label==0.0).takeSample(False, 2406, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_train.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tolabelpoint_2(line):\n",
    "    print line\n",
    "  #return line[1:]\n",
    "    return LabeledPoint(line.label,line.features)\n",
    "sampled_train_label=sampled_train.map(lambda x:tolabelpoint_2(x))\n",
    "sampled_val_label=sampled_val.map(lambda x:tolabelpoint_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_train_label.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load training data\n",
    "#training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# b=from_0_train.collect()[1:10]\n",
    "\n",
    "\n",
    "# c=[LabeledPoint(1.0, [0.574274730517]), LabeledPoint(1.0, [0.401997543364]), LabeledPoint(1.0, [0.401723685504]), LabeledPoint(0.0, [0.332275746599]), LabeledPoint(0.0, [0.351555861186]), LabeledPoint(0.0, [0.44793548048]), LabeledPoint(0.0, [0.330806356856]), LabeledPoint(1.0, [0.513463289702]), LabeledPoint(0.0, [0.37825468551]), LabeledPoint(1.0, [0.541888972034]), LabeledPoint(1.0, [0.306242264246]), LabeledPoint(0.0, [0.363651365059]), LabeledPoint(0.0, [0.416173064627]), LabeledPoint(0.0, [0.452789476752])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training=b\n",
    "# training=c\n",
    "# #display(training)\n",
    "                                                                                                                                \n",
    "# training_df=sqlContext.createDataFrame(training)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lrModel3 = RandomForest.trainClassifier(sampled_train_label, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=7, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "# lrModel3 = model3.fit(training_df)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "# print(\"Coefficients: \" + str(lrModel3.coefficients))\n",
    "# print(\"Intercept: \" + str(lrModel3.intercept))\n",
    "\n",
    "# display(lrModel3.transform(training_df).collect()[0:5])\n",
    "# model3_result=lrModel3.transform(training_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions = lrModel3.predict(sampled_train_label.map(lambda x: x.features))\n",
    "# labelsAndPredictions = sampled_train_label.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "predictions = lrModel3.predict(sampled_val_label.map(lambda x: x.features))\n",
    "labelsAndPredictions = sampled_val_label.map(lambda lp: lp.label).zip(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelsAndPredictions.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=\" %\"\n",
    "acc=round(fin_acc(labelsAndPredictions.collect())*100,2)\n",
    "print(str(acc)+p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actuals=labelsAndPredictions.map(lambda x:x[0]).collect()\n",
    "preds=labelsAndPredictions.map(lambda x:x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actuals[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plt.hist(actuals,bins=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  },
  "name": "million_songs_year_big_all",
  "notebookId": 2620325829840835
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
